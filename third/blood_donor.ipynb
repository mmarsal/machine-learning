{
 "cells": [
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "# Project 3: Blood Donor Classification\n",
    "(by: Martin Marsal, Benedikt Allmendinger, Christian Diegmann; Heilbronn University, Germany, January 2025)"
   ],
   "id": "715e29004ced62fc"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## 0. Preperation\n",
    "First, get to know the dataset and deal with missing values.\n",
    "- Perform an exploratory data analysis to get to know the data set\n",
    "- Preprocess the data. If there are missing values, impute them.\n",
    "- Estimate the accuracy of your imputation for each feature"
   ],
   "id": "1a786bcb66498cdd"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.impute import KNNImputer\n",
    "from sklearn.metrics import mean_squared_error"
   ],
   "id": "f1e5e2d5bbacb547",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# Read the CSV file into a DataFrame\n",
    "df = pd.read_csv('hemodat.csv')\n",
    "\n",
    "# 1. Basic Information\n",
    "print(\"Basic Information:\")\n",
    "print(df.info())\n",
    "print(\"\\nShape of the dataset:\", df.shape)"
   ],
   "id": "1c1586fc9b0d3799",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# 2. Summary Statistics\n",
    "print(\"\\nSummary Statistics (Numerical):\")\n",
    "print(df.describe())\n",
    "\n",
    "print(\"\\nSummary Statistics (Categorical):\")\n",
    "categorical_columns = df.select_dtypes(include=['object']).columns\n",
    "print(df[categorical_columns].describe())"
   ],
   "id": "6a8bf9db324cd42c",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# 3. Distribution Analysis\n",
    "numerical_columns = df.select_dtypes(include=[np.number]).columns\n",
    "\n",
    "# Histograms for numerical data\n",
    "df[numerical_columns].hist(bins=15, figsize=(15, 10), layout=(len(numerical_columns)//3 + 1, 3))\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Count plots for categorical data\n",
    "for col in categorical_columns:\n",
    "    sns.countplot(y=col, data=df)\n",
    "    plt.show()"
   ],
   "id": "2bc1419493f0bfb4",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# 4. Correlation Matrix\n",
    "# Select only numeric columns for correlation matrix\n",
    "numeric_columns = df.select_dtypes(include=[np.number]).columns\n",
    "correlation_matrix = df[numeric_columns].corr()\n",
    "\n",
    "# Visualize the correlation matrix\n",
    "sns.heatmap(correlation_matrix, annot=False, cmap=\"coolwarm\")\n",
    "plt.title(\"Correlation Matrix of Numeric Features\")\n",
    "plt.show()"
   ],
   "id": "628b48bd7c355862",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# 5. Feature Engineering Insights\n",
    "print(\"\\nUnique Values per Column:\")\n",
    "print(df.nunique())"
   ],
   "id": "fe3cdaddebf9b3cb",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# Strip any leading/trailing spaces from column names\n",
    "df.columns = df.columns.str.strip()\n",
    "\n",
    "# Calculate the number of missing values for each feature\n",
    "missing_values = df.isnull().sum()\n",
    "\n",
    "# Output the count of missing values for each feature\n",
    "print(\"Missing values per feature:\")\n",
    "for feature, missing_count in missing_values.items():\n",
    "    print(f\"{feature}: {missing_count}\")"
   ],
   "id": "5c440720cb4ac4fc",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# Define the KNN imputer\n",
    "knn_imputer = KNNImputer(n_neighbors=5, weights='uniform')\n",
    "\n",
    "# Apply KNN imputer to the DataFrame\n",
    "numerical_columns = df.select_dtypes(include=['number']).columns\n",
    "df[numerical_columns] = knn_imputer.fit_transform(df[numerical_columns])\n",
    "\n",
    "# Check if all null values are imputed\n",
    "foundNull = df.isnull().values.any()\n",
    "if foundNull:\n",
    "    raise TypeError('Found null value in DataFrame.')\n",
    "\n",
    "# Output the cleaned DataFrame\n",
    "print(\"DataFrame after KNN imputation:\")\n",
    "print(df)"
   ],
   "id": "95045ed7f537819a",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# Select numerical columns\n",
    "numerical_columns = df.select_dtypes(include=['number']).columns\n",
    "df_numerical = df[numerical_columns]\n",
    "\n",
    "# Create a copy of the original data\n",
    "original_data = df_numerical.copy()\n",
    "\n",
    "# Introduce missing values artificially (10% missing)\n",
    "np.random.seed(42)\n",
    "mask = np.random.rand(*df_numerical.shape) < 0.1  # Mask for 10% missing\n",
    "df_missing = df_numerical.copy()\n",
    "df_missing[mask] = np.nan\n",
    "\n",
    "# Apply KNN Imputer\n",
    "knn_imputer = KNNImputer(n_neighbors=5, weights='uniform')\n",
    "df_imputed = pd.DataFrame(knn_imputer.fit_transform(df_missing), columns=numerical_columns)\n",
    "\n",
    "# Compare imputed values with original values\n",
    "print(f\"Mean Squared Error for each feature:\")\n",
    "for col in numerical_columns:\n",
    "    # Calculate error only for artificially missing values\n",
    "    mask_col = mask[:, df_numerical.columns.get_loc(col)]\n",
    "    mse = mean_squared_error(original_data[col][mask_col], df_imputed[col][mask_col])\n",
    "    print(f\"'{col}': {mse}\")"
   ],
   "id": "5c9ac104958c4701",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## 1. Anomaly Detection\n",
    "Since medical conditions that lead to the rejection of a donor are rare (luckily) and can be very\n",
    "versatile. It is near impossible to categorize every possible condition. Hence, it would be useful to have an anomaly\n",
    "detection algorithm in place as a safety mechanism to detect suspicious blood samples for further testing.\n",
    "- Train an anomaly detection model based only on valid blood donors without a medical condition.\n",
    "- Evaluate the accuracy of your anomaly detection by testing it also on donors with a medical condition.\n",
    "- Perform a PCA to visualize the true / false positive and true / false negative predictions as well as the decision\n",
    "boundary of your anomaly detection. How much variance is explained by the first two main components? "
   ],
   "id": "d7f9fd9a8724a94b"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# df_blood_donors = df[df['Category'] == '0=Blood Donor']\n",
    "# \n",
    "# from sklearn.preprocessing import StandardScaler\n",
    "# \n",
    "# # Select numerical columns for standardization\n",
    "# numerical_cols = df_blood_donors.select_dtypes(include=['float64', 'int64']).columns\n",
    "# \n",
    "# scaler = StandardScaler()\n",
    "# df_blood_donors.loc[:, numerical_cols] = scaler.fit_transform(df_blood_donors[numerical_cols])\n",
    "# \n",
    "# print(df_blood_donors)"
   ],
   "id": "69e436d536523b8",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# from sklearn.ensemble import IsolationForest\n",
    "# \n",
    "# # Initialize and train the anomaly detection model\n",
    "# model = IsolationForest(contamination=0.05)  # Assuming 5% contamination (anomalies)\n",
    "# model.fit(df_blood_donors[numerical_cols])\n",
    "# \n",
    "# # Predict anomalies\n",
    "# anomalies = model.predict(df_blood_donors[numerical_cols])\n",
    "# \n",
    "# # Anomalies will be -1 (outliers) or 1 (normal)\n",
    "# df_blood_donors = df[df['Category'] == '0=Blood Donor'].copy()  # Create a deep copy\n",
    "# df_blood_donors['anomaly'] = anomalies  # No warning here\n",
    "# \n",
    "# print(df_blood_donors)"
   ],
   "id": "b282141ef5ebe3b7",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# df_medical_conditions = df[df['Category'] != '0=Blood Donor']\n",
    "# # test_data = pd.concat([df_blood_donors, df_medical_conditions])\n",
    "# \n",
    "# test_data = df[df['Category'] != '0=Blood Donor'].copy()\n",
    "# \n",
    "# # Preprocess the test data similarly to the training data\n",
    "# test_data[numerical_cols] = scaler.transform(test_data[numerical_cols])\n",
    "# \n",
    "# test_data['predictions'] = model.predict(test_data[numerical_cols])\n",
    "# print(test_data)"
   ],
   "id": "a42d16e4c3f8c378",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "from sklearn.ensemble import IsolationForest\n",
    "from sklearn.metrics import classification_report\n",
    "from sklearn.decomposition import PCA\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Filter data for valid blood donors\n",
    "donors = df[df['Category'] == '0=Blood Donor']\n",
    "\n",
    "# Filter data for non-donors (anomalous category)\n",
    "non_donors = df[df['Category'] != '0=Blood Donor']\n",
    "\n",
    "# Numerical columns for modeling\n",
    "numerical_columns = ['ALB', 'ALP', 'ALT', 'AST', 'BIL', 'CHE', 'CHOL', 'CREA', 'GGT', 'PROT']\n",
    "\n",
    "# Features for anomaly detection\n",
    "X_train = donors[numerical_columns]  # Train on valid donors only\n",
    "X_test = non_donors[numerical_columns]  # Use non-donors for testing\n",
    "\n",
    "# Define and train Isolation Forest\n",
    "iso_forest = IsolationForest(random_state=42, contamination=0.1)  # Assuming 10% contamination\n",
    "iso_forest.fit(X_train)\n",
    "\n",
    "# Predict anomalies on the combined dataset\n",
    "combined_data = pd.concat([donors, non_donors], axis=0)\n",
    "combined_features = combined_data[numerical_columns]\n",
    "predictions = iso_forest.predict(combined_features)\n",
    "\n",
    "# Map predictions to binary format (1: normal, -1: anomaly)\n",
    "pred_binary = np.where(predictions == 1, 1, 0)  # 1: Valid, 0: Anomaly\n",
    "true_labels = np.where(combined_data['Category'] == '0=Blood Donor', 1, -1)\n",
    "true_binary = np.where(true_labels == 1, 1, 0)\n",
    "\n",
    "# Classification report\n",
    "print(\"Classification Report:\")\n",
    "print(classification_report(true_binary, pred_binary, target_names=['Anomaly', 'Valid']))\n",
    "\n",
    "# Calculate the counts of true/false positives and negatives\n",
    "tp = np.sum((true_binary == 1) & (pred_binary == 1))  # True Positives\n",
    "fp = np.sum((true_binary == 0) & (pred_binary == 1))  # False Positives\n",
    "tn = np.sum((true_binary == 0) & (pred_binary == 0))  # True Negatives\n",
    "fn = np.sum((true_binary == 1) & (pred_binary == 0))  # False Negatives\n",
    "\n",
    "# Display the counts\n",
    "print(f\"Counts:\")\n",
    "print(f\"True Positives (TP): {tp}\")\n",
    "print(f\"False Positives (FP): {fp}\")\n",
    "print(f\"True Negatives (TN): {tn}\")\n",
    "print(f\"False Negatives (FN): {fn}\")\n",
    "\n",
    "# Perform PCA\n",
    "pca = PCA(n_components=2)\n",
    "pca_data = pca.fit_transform(combined_features)\n",
    "\n",
    "# Variance explained by PCA components\n",
    "variance_explained = pca.explained_variance_ratio_\n",
    "print(f\"Variance explained by the first two components: {variance_explained}\")\n",
    "\n",
    "# Scatter plot with True/False Positives and Negatives\n",
    "plt.figure(figsize=(12, 8))\n",
    "plt.scatter(pca_data[(true_binary == 1) & (pred_binary == 1), 0], pca_data[(true_binary == 1) & (pred_binary == 1), 1], \n",
    "            c='green', label='True Positives', alpha=0.6, edgecolor='k')\n",
    "plt.scatter(pca_data[(true_binary == 0) & (pred_binary == 1), 0], pca_data[(true_binary == 0) & (pred_binary == 1), 1], \n",
    "            c='orange', label='False Positives', alpha=0.6, edgecolor='k')\n",
    "plt.scatter(pca_data[(true_binary == 0) & (pred_binary == 0), 0], pca_data[(true_binary == 0) & (pred_binary == 0), 1], \n",
    "            c='blue', label='True Negatives', alpha=0.6, edgecolor='k')\n",
    "plt.scatter(pca_data[(true_binary == 1) & (pred_binary == 0), 0], pca_data[(true_binary == 1) & (pred_binary == 0), 1], \n",
    "            c='red', label='False Negatives', alpha=0.6, edgecolor='k')\n",
    "plt.title(\"PCA Visualization of Predictions\")\n",
    "plt.xlabel(f\"Principal Component 1 ({variance_explained[0]:.2%} variance)\")\n",
    "plt.ylabel(f\"Principal Component 2 ({variance_explained[1]:.2%} variance)\")\n",
    "plt.legend()\n",
    "plt.grid(True)\n",
    "plt.show()\n",
    "\n",
    "# Decision boundary visualization using PCA components\n",
    "xx, yy = np.meshgrid(\n",
    "    np.linspace(pca_data[:, 0].min() - 1, pca_data[:, 0].max() + 1, 100),\n",
    "    np.linspace(pca_data[:, 1].min() - 1, pca_data[:, 1].max() + 1, 100)\n",
    ")\n",
    "# Prepare the grid points for prediction\n",
    "mesh_points = np.c_[xx.ravel(), yy.ravel()]\n",
    "inverse_transformed = pca.inverse_transform(mesh_points)\n",
    "\n",
    "# Ensure the mesh_points (inverse_transformed) have valid feature names\n",
    "inverse_transformed_df = pd.DataFrame(inverse_transformed, columns=numerical_columns)\n",
    "\n",
    "# Predict anomaly scores for the mesh points\n",
    "mesh_scores = iso_forest.decision_function(inverse_transformed_df).reshape(xx.shape)\n",
    "\n",
    "# Plot decision boundary\n",
    "plt.figure(figsize=(12, 8))\n",
    "plt.contourf(xx, yy, mesh_scores, levels=50, cmap='coolwarm', alpha=0.5)\n",
    "plt.colorbar(label='Anomaly Score')\n",
    "plt.scatter(pca_data[(true_binary == 1) & (pred_binary == 1), 0], pca_data[(true_binary == 1) & (pred_binary == 1), 1], \n",
    "            c='green', label='True Positives', alpha=0.6, edgecolor='k')\n",
    "plt.scatter(pca_data[(true_binary == 0) & (pred_binary == 1), 0], pca_data[(true_binary == 0) & (pred_binary == 1), 1], \n",
    "            c='orange', label='False Positives', alpha=0.6, edgecolor='k')\n",
    "plt.scatter(pca_data[(true_binary == 0) & (pred_binary == 0), 0], pca_data[(true_binary == 0) & (pred_binary == 0), 1], \n",
    "            c='blue', label='True Negatives', alpha=0.6, edgecolor='k')\n",
    "plt.scatter(pca_data[(true_binary == 1) & (pred_binary == 0), 0], pca_data[(true_binary == 1) & (pred_binary == 0), 1], \n",
    "            c='red', label='False Negatives', alpha=0.6, edgecolor='k')\n",
    "plt.title(\"PCA Visualization with Decision Boundary\")\n",
    "plt.xlabel(f\"Principal Component 1 ({variance_explained[0]:.2%} variance)\")\n",
    "plt.ylabel(f\"Principal Component 2 ({variance_explained[1]:.2%} variance)\")\n",
    "plt.legend()\n",
    "plt.grid(True)\n",
    "plt.show()"
   ],
   "id": "cc415f1b050c363a",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## 2. Explainable Model\n",
    "For your decision support your model should be explainable. Train a model with a focus on\n",
    "explainability with an as simple as possible structure while still maintaining its predictive power.\n",
    "- Train a decision tree classifier on the imputed data. Evaluate your model’s accuracy and visualize the tree structure to\n",
    "help the hospital personal understand the decision process. Each inference should not only put out the class, but also\n",
    "the decision path taken. Make the tree as simple and understandable as possible."
   ],
   "id": "aa5c964496020d22"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "",
   "id": "f8f169f92c70d7c7",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## 3. High Performance Model\n",
    "This time the focus is on predictive power. Try and train a more accurate model. Is it worth\n",
    "the effort?\n",
    "- Train and optimize an XGBoost classifier on the imputed data.\n",
    "- Use SHAP local explanation techniques on 5 selected data points and discuss the results\n",
    "- Use SHAP global explanation techniques to visualize and discuss the influence of different features.\n",
    "- Evaluate the XGBoost’s accuracy and compare it to the Decision Tree"
   ],
   "id": "cb3afb52034965f9"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "",
   "id": "bf12497641e62b54",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## 4. Combined Model\n",
    "Put all components into a single model artifact for deployment such that clinic personal has all important\n",
    "information at hand to make an informed decision.\n",
    "- Combine the XGBoost, Decision Tree and Anomaly Detection in a single model class including all necessary methods (fit,\n",
    "predict…). The Decision Tree provides an explainable assistance for the hospital personal and the XGBoost (probably) a more\n",
    "accurate classification. The Anomaly Detection increases the robustness of the model for conditions that have not been\n",
    "explicitly trained or for human errors. Generate a few test anomalies to check your detection.\n",
    "- Evaluate, discuss and plot the performance of your combined model."
   ],
   "id": "ae4e4bb54a7a19c2"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "",
   "id": "6aa0fc87d41c2136",
   "outputs": [],
   "execution_count": null
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
